{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3486tga6LY-",
        "outputId": "7a895924-71cf-4cba-e88e-b1f3b47bf9a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ibm-watsonx-ai\n",
            "  Downloading ibm_watsonx_ai-1.4.1-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (2024.11.6)\n",
            "Collecting regex\n",
            "  Downloading regex-2025.9.18-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from ibm-watsonx-ai) (2.32.4)\n",
            "Requirement already satisfied: httpx<0.29,>=0.27 in /usr/local/lib/python3.12/dist-packages (from ibm-watsonx-ai) (0.28.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.12/dist-packages (from ibm-watsonx-ai) (2.5.0)\n",
            "Requirement already satisfied: pandas<2.3.0,>=0.24.2 in /usr/local/lib/python3.12/dist-packages (from ibm-watsonx-ai) (2.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from ibm-watsonx-ai) (2025.10.5)\n",
            "Collecting lomond (from ibm-watsonx-ai)\n",
            "  Downloading lomond-0.3.3-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from ibm-watsonx-ai) (0.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from ibm-watsonx-ai) (25.0)\n",
            "Collecting ibm-cos-sdk<2.15.0,>=2.12.0 (from ibm-watsonx-ai)\n",
            "  Downloading ibm_cos_sdk-2.14.3.tar.gz (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from ibm-watsonx-ai) (5.5.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.35.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<0.29,>=0.27->ibm-watsonx-ai) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<0.29,>=0.27->ibm-watsonx-ai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<0.29,>=0.27->ibm-watsonx-ai) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<0.29,>=0.27->ibm-watsonx-ai) (0.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\n",
            "Collecting ibm-cos-sdk-core==2.14.3 (from ibm-cos-sdk<2.15.0,>=2.12.0->ibm-watsonx-ai)\n",
            "  Downloading ibm_cos_sdk_core-2.14.3.tar.gz (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ibm-cos-sdk-s3transfer==2.14.3 (from ibm-cos-sdk<2.15.0,>=2.12.0->ibm-watsonx-ai)\n",
            "  Downloading ibm_cos_sdk_s3transfer-2.14.3.tar.gz (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.6/139.6 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jmespath<=1.0.1,>=0.10.0 (from ibm-cos-sdk<2.15.0,>=2.12.0->ibm-watsonx-ai)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.9.0 in /usr/local/lib/python3.12/dist-packages (from ibm-cos-sdk-core==2.14.3->ibm-cos-sdk<2.15.0,>=2.12.0->ibm-watsonx-ai) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<2.3.0,>=0.24.2->ibm-watsonx-ai) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<2.3.0,>=0.24.2->ibm-watsonx-ai) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->ibm-watsonx-ai) (3.4.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from lomond->ibm-watsonx-ai) (1.17.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<0.29,>=0.27->ibm-watsonx-ai) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Downloading ibm_watsonx_ai-1.4.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading regex-2025.9.18-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (802 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.0/802.0 kB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lomond-0.3.3-py2.py3-none-any.whl (35 kB)\n",
            "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Building wheels for collected packages: ibm-cos-sdk, ibm-cos-sdk-core, ibm-cos-sdk-s3transfer\n",
            "  Building wheel for ibm-cos-sdk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ibm-cos-sdk: filename=ibm_cos_sdk-2.14.3-py3-none-any.whl size=77232 sha256=c1ef429f2a74cd9226a26f035e992d741421220fb7496de8e1669854cdfc4767\n",
            "  Stored in directory: /root/.cache/pip/wheels/cc/2f/6f/125918ad46d280d3bea58edf99f0757888ef6e7999db4b73b7\n",
            "  Building wheel for ibm-cos-sdk-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ibm-cos-sdk-core: filename=ibm_cos_sdk_core-2.14.3-py3-none-any.whl size=662101 sha256=3309e397879888f19293ad921fdd1a1ca2ca1f4a90922d79688a1d986e26f6e3\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/53/13/7c8fdeebdb847995d8ef349b4f695c595d8d31b30ae2a07ea2\n",
            "  Building wheel for ibm-cos-sdk-s3transfer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ibm-cos-sdk-s3transfer: filename=ibm_cos_sdk_s3transfer-2.14.3-py3-none-any.whl size=90203 sha256=936a66cf03ed81700530519f6ceb3450e3c8b6c0e8c766f626bf94c35eb40d16\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/8b/10/0346c5a955b48b7fca50a8a42de309546b22899b7e8c1da8a5\n",
            "Successfully built ibm-cos-sdk ibm-cos-sdk-core ibm-cos-sdk-s3transfer\n",
            "Installing collected packages: regex, lomond, jmespath, ibm-cos-sdk-core, ibm-cos-sdk-s3transfer, ibm-cos-sdk, ibm-watsonx-ai\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2024.11.6\n",
            "    Uninstalling regex-2024.11.6:\n",
            "      Successfully uninstalled regex-2024.11.6\n",
            "Successfully installed ibm-cos-sdk-2.14.3 ibm-cos-sdk-core-2.14.3 ibm-cos-sdk-s3transfer-2.14.3 ibm-watsonx-ai-1.4.1 jmespath-1.0.1 lomond-0.3.3 regex-2025.9.18\n"
          ]
        }
      ],
      "source": [
        "!pip install -U ibm-watsonx-ai sentence-transformers transformers regex\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# COLAB: Tweet summary + company & drug extraction (watsonx)\n",
        "# - Uses local file already in Colab workspace\n",
        "# - Includes therapy classes like \"chemotherapy\" and \"immunotherapy\"\n",
        "# ============================================================\n",
        "\n",
        "# 0) Install minimal deps\n",
        "%pip install -U ibm-watsonx-ai regex\n",
        "\n",
        "# 1) ---- HARD-CODE YOUR CREDENTIALS & MODEL ----\n",
        "WATSONX_API_KEY = \"B5EQe7EQckE5N1eLMtaAfGi1Gr9rOqGjyyGLN8MMispB\"\n",
        "WATSONX_URL     = \"https://us-south.ml.cloud.ibm.com\"   # adjust if your region differs\n",
        "# Use ONE of these. If both are filled, SPACE_ID will be used.\n",
        "WATSONX_PROJECT_ID = \"\"  # e.g., \"250bddc8-62e9-447d-b4cd-fffba8bfff05\"\n",
        "WATSONX_SPACE_ID   = \"1f66bc7c-f805-476f-92f6-1d6ebd561f15\"\n",
        "\n",
        "MODEL_ID = \"meta-llama/llama-3-3-70b-instruct\"  # chat_model on watsonx\n",
        "\n",
        "# 2) ---- RUNTIME SETTINGS ----\n",
        "TEMPERATURE = 0.3\n",
        "TOP_P = 0.9\n",
        "MAX_NEW_TOKENS = 300\n",
        "\n",
        "# 3) ---- IMPORTS & MODEL SETUP ----\n",
        "import os, json, re, os.path\n",
        "from typing import List, Dict, Optional\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "assert WATSONX_API_KEY and WATSONX_URL and (WATSONX_PROJECT_ID or WATSONX_SPACE_ID), \\\n",
        "    \"Missing WATSONX_API_KEY/WATSONX_URL and either PROJECT_ID or SPACE_ID.\"\n",
        "\n",
        "os.environ[\"WATSONX_API_KEY\"]  = WATSONX_API_KEY\n",
        "os.environ[\"WATSONX_URL\"]      = WATSONX_URL\n",
        "if WATSONX_SPACE_ID:\n",
        "    os.environ[\"WATSONX_SPACE_ID\"]   = WATSONX_SPACE_ID\n",
        "    os.environ.pop(\"WATSONX_PROJECT_ID\", None)\n",
        "else:\n",
        "    os.environ[\"WATSONX_PROJECT_ID\"] = WATSONX_PROJECT_ID\n",
        "    os.environ.pop(\"WATSONX_SPACE_ID\", None)\n",
        "\n",
        "from ibm_watsonx_ai import Credentials\n",
        "from ibm_watsonx_ai.foundation_models import Model\n",
        "\n",
        "creds = Credentials(url=os.environ[\"WATSONX_URL\"], api_key=os.environ[\"WATSONX_API_KEY\"])\n",
        "params = {\"decoding_method\":\"sample\",\"temperature\":float(TEMPERATURE),\"top_p\":float(TOP_P),\"max_new_tokens\":int(MAX_NEW_TOKENS)}\n",
        "\n",
        "mdl_kwargs = {}\n",
        "if os.getenv(\"WATSONX_SPACE_ID\"):\n",
        "    mdl_kwargs[\"space_id\"] = os.environ[\"WATSONX_SPACE_ID\"]\n",
        "else:\n",
        "    mdl_kwargs[\"project_id\"] = os.environ[\"WATSONX_PROJECT_ID\"]\n",
        "\n",
        "wxa_model = Model(model_id=MODEL_ID, credentials=creds, params=params, **mdl_kwargs)\n",
        "\n",
        "def watsonx_generate(prompt: str) -> str:\n",
        "    \"\"\"Try chat; fall back to non-chat generate_text; return plain string.\"\"\"\n",
        "    try:\n",
        "        if hasattr(wxa_model, \"start_chat\"):\n",
        "            chat = wxa_model.start_chat()\n",
        "            resp = chat.send_message(prompt)\n",
        "            text = getattr(resp, \"message\", None) or getattr(resp, \"generated_text\", None)\n",
        "            if isinstance(text, str) and text.strip():\n",
        "                return text\n",
        "            if isinstance(resp, dict):\n",
        "                return resp.get(\"generated_text\") or resp.get(\"message\") or json.dumps(resp)\n",
        "    except Exception:\n",
        "        pass\n",
        "    out = wxa_model.generate_text(prompt=prompt)\n",
        "    if isinstance(out, str):\n",
        "        return out\n",
        "    if isinstance(out, dict):\n",
        "        if \"results\" in out and out[\"results\"]:\n",
        "            cand = out[\"results\"][0].get(\"generated_text\") or out[\"results\"][0].get(\"text\")\n",
        "            if cand:\n",
        "                return cand\n",
        "        return out.get(\"generated_text\") or json.dumps(out)\n",
        "    return str(out)\n",
        "\n",
        "# 4) ---- HELPERS ----\n",
        "def trim_tweet(text: str, max_chars: int = 280) -> str:\n",
        "    t = re.sub(r\"\\s+\", \" \", text.strip())\n",
        "    if len(t) <= max_chars:\n",
        "        return t\n",
        "    cut = t[:max_chars]\n",
        "    idx = max(cut.rfind(\".\"), cut.rfind(\";\"), cut.rfind(\",\"), cut.rfind(\" \"))\n",
        "    if idx > max_chars * 0.6:\n",
        "        return cut[:idx].rstrip()\n",
        "    return cut.rstrip()\n",
        "\n",
        "URL_COMPANY_MAP = {\n",
        "    \"gene.com\": \"Genentech\",\n",
        "    \"roche.com\": \"Roche\",\n",
        "    \"novartis.com\": \"Novartis\",\n",
        "    \"pfizer.com\": \"Pfizer\",\n",
        "    \"astrazeneca.com\": \"AstraZeneca\",\n",
        "    \"lilly.com\": \"Eli Lilly\",\n",
        "    \"merck.com\": \"Merck\",\n",
        "    \"sanofi.com\": \"Sanofi\",\n",
        "    \"gilead.com\": \"Gilead\",\n",
        "    \"bms.com\": \"BMS\",\n",
        "    \"amgen.com\": \"Amgen\",\n",
        "    \"gsk.com\": \"GSK\",\n",
        "    \"bayer.com\": \"Bayer\",\n",
        "    \"takeda.com\": \"Takeda\",\n",
        "    \"boehringer-ingelheim.com\": \"Boehringer Ingelheim\",\n",
        "    \"beigene.com\": \"BeiGene\",\n",
        "    \"seagen.com\": \"Seagen\",\n",
        "    \"sermonixpharma.com\": \"Sermonix Pharma\",\n",
        "}\n",
        "KNOWN_PHARMA = {\n",
        "    \"Genentech\",\"Roche\",\"Novartis\",\"Pfizer\",\"AstraZeneca\",\"Eli Lilly\",\"Merck\",\n",
        "    \"Sanofi\",\"Gilead\",\"BMS\",\"Amgen\",\"GSK\",\"Bayer\",\"Takeda\",\"Boehringer Ingelheim\",\n",
        "    \"BeiGene\",\"Seagen\",\"Sermonix Pharma\",\"Sermonix\"\n",
        "}\n",
        "ORG_SUFFIXES = r\"(?: Inc\\.?| Corp\\.?| Corporation| Ltd\\.?| LLC| plc| AG| SA| NV| Co\\.?)\"\n",
        "\n",
        "def company_from_url(url: str) -> Optional[str]:\n",
        "    try:\n",
        "        host = urlparse(url).netloc.lower()\n",
        "        for prefix in (\"www.\", \"amp.\", \"m.\", \"news.\"):\n",
        "            if host.startswith(prefix):\n",
        "                host = host[len(prefix):]\n",
        "        if host in URL_COMPANY_MAP:\n",
        "            return URL_COMPANY_MAP[host]\n",
        "        parts = host.split(\".\")\n",
        "        brand = parts[-2] if len(parts) >= 2 else parts[0]\n",
        "        brand = brand.replace(\"-\", \" \").strip()\n",
        "        return brand.capitalize() if brand else None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def find_companies_from_text(text: str) -> List[str]:\n",
        "    found = set()\n",
        "    for k in KNOWN_PHARMA:\n",
        "        if re.search(rf\"\\b{k}\\b\", text, flags=re.I):\n",
        "            found.add(k)\n",
        "    for m in re.finditer(rf\"\\b([A-Z][A-Za-z&\\-]+(?:\\s+[A-Z][A-Za-z&\\-]+){{0,2}})(?:{ORG_SUFFIXES})?\\b\", text):\n",
        "        name = m.group(0).strip()\n",
        "        if name.lower() in {\"q\", \"fda\", \"phase\", \"trial\", \"study\", \"breast\", \"cancer\"}:\n",
        "            continue\n",
        "        found.add(name)\n",
        "    out = [re.sub(r\"\\s+\", \" \", f).strip() for f in found]\n",
        "    return list({x.lower(): x for x in out}.values())\n",
        "\n",
        "# Therapy classes we’re allowed to include if they appear verbatim\n",
        "THERAPY_CLASSES = [\n",
        "    \"chemotherapy\",\n",
        "    \"immunotherapy\",\n",
        "    # add more if you want:\n",
        "    # \"endocrine therapy\", \"hormonal therapy\", \"targeted therapy\",\n",
        "]\n",
        "\n",
        "def llm_extract_drugs_strict(headline: str, content: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Extract drug/product names verbatim (including therapy classes like\n",
        "    'chemotherapy' and 'immunotherapy' if they appear in the text).\n",
        "    \"\"\"\n",
        "    snippet = (headline + \"\\n\" + content).strip()\n",
        "    if len(snippet) > 5000:\n",
        "        snippet = snippet[:5000]\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Extract drug, product, or therapy names that appear **verbatim** in the provided text.\n",
        "\n",
        "RULES:\n",
        "- Return ONLY names that exist in the text (headline or content).\n",
        "- If a brand and generic appear like \"Itovebi (inavolisib)\", return both items separately.\n",
        "- INCLUDE therapy classes when they appear (e.g., \"chemotherapy\", \"immunotherapy\").\n",
        "- Return a pure JSON array of strings (no commentary).\n",
        "\n",
        "TEXT:\n",
        "\\\"\\\"\\\"{snippet}\\\"\\\"\\\"\n",
        "\n",
        "Return JSON array:\n",
        "\"\"\"\n",
        "    raw = watsonx_generate(prompt).strip()\n",
        "    match = re.search(r\"\\[\\s*(?:\\\".*?\\\")\\s*(?:,.*?)*\\]\", raw, flags=re.S)\n",
        "    items = []\n",
        "    if match:\n",
        "        try:\n",
        "            items = json.loads(match.group(0))\n",
        "        except Exception:\n",
        "            items = []\n",
        "    if not isinstance(items, list):\n",
        "        items = []\n",
        "    items = [s.strip() for s in items if isinstance(s, str) and s.strip()]\n",
        "\n",
        "    # Validate: keep only names that literally appear in the text (normalize ™/®)\n",
        "    low = snippet.lower()\n",
        "    validated = []\n",
        "    for name in items:\n",
        "        norm = re.sub(r\"[™®]\", \"\", name).strip()\n",
        "        if norm and (norm.lower() in low):\n",
        "            validated.append(name)\n",
        "\n",
        "    # Ensure therapy classes are included if present in text, even if the LLM missed them\n",
        "    for cls in THERAPY_CLASSES:\n",
        "        if cls.lower() in low and cls not in validated:\n",
        "            validated.append(cls)\n",
        "\n",
        "    # Dedup preserve order\n",
        "    seen = set(); final = []\n",
        "    for x in validated:\n",
        "        k = x.lower()\n",
        "        if k not in seen:\n",
        "            seen.add(k); final.append(x)\n",
        "    return final\n",
        "\n",
        "def llm_summary_tweet(headline: str, content: str) -> str:\n",
        "    snippet = (headline + \"\\n\" + content).strip()\n",
        "    if len(snippet) > 6000:\n",
        "        snippet = snippet[:6000]\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Write ONE tweet-style summary (<= 280 characters) strictly based on the article text below.\n",
        "REQUIREMENTS:\n",
        "- Mention concrete specifics present in the text (e.g., drug name(s), indication, setting/line, and endpoint if mentioned).\n",
        "- No emojis, no hashtags, no marketing language, no invented details.\n",
        "- One concise sentence or two short clauses.\n",
        "\n",
        "TEXT:\n",
        "\\\"\\\"\\\"{snippet}\\\"\\\"\\\"\n",
        "\n",
        "Tweet (<=280 chars):\n",
        "\"\"\"\n",
        "    raw = watsonx_generate(prompt)\n",
        "    tweet = raw.splitlines()[0].strip() if raw else \"\"\n",
        "    return trim_tweet(tweet, max_chars=280)\n",
        "\n",
        "def process_articles(articles: List[Dict]) -> List[Dict]:\n",
        "    results = []\n",
        "    for art in articles:\n",
        "        headline = (art.get(\"headline\") or \"\").strip()\n",
        "        url = (art.get(\"url\") or \"\").strip()\n",
        "        content = (art.get(\"content\") or \"\").strip()\n",
        "        text_all = (headline + \"\\n\" + content).strip()\n",
        "\n",
        "        summary = llm_summary_tweet(headline, content)\n",
        "\n",
        "        companies = []\n",
        "        from_url = company_from_url(url)\n",
        "        if from_url:\n",
        "            companies.append(from_url)\n",
        "        companies += find_companies_from_text(text_all)\n",
        "\n",
        "        filtered = []\n",
        "        for c in companies:\n",
        "            c_norm = c.strip()\n",
        "            if not c_norm or c_norm in filtered:\n",
        "                continue\n",
        "            if (c_norm in KNOWN_PHARMA) or (c_norm == from_url) or re.search(ORG_SUFFIXES + r\"$\", c_norm):\n",
        "                filtered.append(c_norm)\n",
        "\n",
        "        drugs = llm_extract_drugs_strict(headline, content)\n",
        "\n",
        "        results.append({\n",
        "            \"headline\": headline,\n",
        "            \"url\": url,\n",
        "            \"summary_tweet\": summary,\n",
        "            \"companies\": filtered,\n",
        "            \"drugs\": drugs\n",
        "        })\n",
        "    return results\n",
        "\n",
        "print(\"✅ watsonx model ready:\", MODEL_ID)\n",
        "\n",
        "# 5) ---- LOAD INPUT JSON FROM LOCAL FILE IN COLAB ----\n",
        "# If your file is already in the working directory as 'breast_cancer_news_content' (with or without .json):\n",
        "base_name = \"breast_cancer_news_content\"\n",
        "candidates = [base_name, base_name + \".json\", base_name + \".JSON\"]\n",
        "INPUT_PATH = None\n",
        "for p in candidates:\n",
        "    if os.path.exists(p):\n",
        "        INPUT_PATH = p\n",
        "        break\n",
        "if INPUT_PATH is None:\n",
        "    raise FileNotFoundError(\"Could not find 'breast_cancer_news_content' (with or without .json/.JSON) in the current folder.\")\n",
        "\n",
        "print(\"Reading:\", INPUT_PATH)\n",
        "with open(INPUT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    articles_in = json.load(f)\n",
        "\n",
        "# 6) ---- RUN & SAVE OUTPUT ----\n",
        "processed = process_articles(articles_in)\n",
        "\n",
        "OUTPUT_PATH = \"article_summaries_extractions.json\"\n",
        "with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(processed, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"Sample output preview:\")\n",
        "print(json.dumps(processed[:1], ensure_ascii=False, indent=2))\n",
        "\n",
        "# 7) ---- DOWNLOAD ----\n",
        "from google.colab import files\n",
        "files.download(OUTPUT_PATH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        },
        "id": "7YAZR-4B-gJH",
        "outputId": "a1f7d1a2-a574-43f4-af30-ece2bf8785c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ibm-watsonx-ai in /usr/local/lib/python3.12/dist-packages (1.4.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (2025.9.18)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from ibm-watsonx-ai) (2.32.4)\n",
            "Requirement already satisfied: httpx<0.29,>=0.27 in /usr/local/lib/python3.12/dist-packages (from ibm-watsonx-ai) (0.28.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.12/dist-packages (from ibm-watsonx-ai) (2.5.0)\n",
            "Requirement already satisfied: pandas<2.3.0,>=0.24.2 in /usr/local/lib/python3.12/dist-packages (from ibm-watsonx-ai) (2.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from ibm-watsonx-ai) (2025.10.5)\n",
            "Requirement already satisfied: lomond in /usr/local/lib/python3.12/dist-packages (from ibm-watsonx-ai) (0.3.3)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from ibm-watsonx-ai) (0.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from ibm-watsonx-ai) (25.0)\n",
            "Requirement already satisfied: ibm-cos-sdk<2.15.0,>=2.12.0 in /usr/local/lib/python3.12/dist-packages (from ibm-watsonx-ai) (2.14.3)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from ibm-watsonx-ai) (5.5.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<0.29,>=0.27->ibm-watsonx-ai) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<0.29,>=0.27->ibm-watsonx-ai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<0.29,>=0.27->ibm-watsonx-ai) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<0.29,>=0.27->ibm-watsonx-ai) (0.16.0)\n",
            "Requirement already satisfied: ibm-cos-sdk-core==2.14.3 in /usr/local/lib/python3.12/dist-packages (from ibm-cos-sdk<2.15.0,>=2.12.0->ibm-watsonx-ai) (2.14.3)\n",
            "Requirement already satisfied: ibm-cos-sdk-s3transfer==2.14.3 in /usr/local/lib/python3.12/dist-packages (from ibm-cos-sdk<2.15.0,>=2.12.0->ibm-watsonx-ai) (2.14.3)\n",
            "Requirement already satisfied: jmespath<=1.0.1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from ibm-cos-sdk<2.15.0,>=2.12.0->ibm-watsonx-ai) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.9.0 in /usr/local/lib/python3.12/dist-packages (from ibm-cos-sdk-core==2.14.3->ibm-cos-sdk<2.15.0,>=2.12.0->ibm-watsonx-ai) (2.9.0.post0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas<2.3.0,>=0.24.2->ibm-watsonx-ai) (2.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<2.3.0,>=0.24.2->ibm-watsonx-ai) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<2.3.0,>=0.24.2->ibm-watsonx-ai) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->ibm-watsonx-ai) (3.4.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from lomond->ibm-watsonx-ai) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<0.29,>=0.27->ibm-watsonx-ai) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<0.29,>=0.27->ibm-watsonx-ai) (4.15.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/ibm_watsonx_ai/foundation_models/model.py:106: DeprecationWarning: The `Model` class is deprecated and will be removed in a future release. Please use the `ModelInference` class instead. To update your imports, use: `from ibm_watsonx_ai.foundation_models import ModelInference`.\n",
            "  warn(model_class_deprecated_warning, category=DeprecationWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ watsonx model ready: meta-llama/llama-3-3-70b-instruct\n",
            "Reading: breast_cancer_news_content.json\n",
            "Sample output preview:\n",
            "[\n",
            "  {\n",
            "    \"headline\": \"FDA Approves New Targeted Treatment For Advanced Hormone Receptor-Positive, HER2-Negative Breast Cancer With A PIK3CA Mutation\",\n",
            "    \"url\": \"https://www.gene.com/media/news-features/fda-approves-new-targeted-treatment-for-advanced-hormone-receptor-positive-her2-negative-breast-cancer-with-a-pik3ca-mutation\",\n",
            "    \"summary_tweet\": \"\\\"The FDA approved Itovebi (inavolisib) in combination with palbociclib and fulvestrant for first-line treatment of HR+, HER2-, PIK3CA-mutated metastatic breast cancer, doubling progression-free survival in the Phase III INAVO120 trial.\\\"\",\n",
            "    \"companies\": [\n",
            "      \"Genentech\"\n",
            "    ],\n",
            "    \"drugs\": [\n",
            "      \"Itovebi\",\n",
            "      \"inavolisib\",\n",
            "      \"palbociclib\",\n",
            "      \"fulvestrant\",\n",
            "      \"targeted therapy\"\n",
            "    ]\n",
            "  }\n",
            "]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_80b13008-e278-44ca-8bf3-197c72c3f9bd\", \"article_summaries_extractions.json\", 17611)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entity Extraction + Summarization Agent"
      ],
      "metadata": {
        "id": "8cZSbyotukCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# COLAB: Article extraction (watsonx) — strict output schema + incremental writes\n",
        "# - Outputs ONLY requested fields\n",
        "# - Company from URL only\n",
        "# - Drug names exclude therapy classes\n",
        "# - Trial names only proper names (e.g., KEYNOTE-522)\n",
        "# - Incremental NDJSON + rolling JSON snapshot\n",
        "# ============================================================\n",
        "\n",
        "# 0) Install minimal deps\n",
        "%pip install -U ibm-watsonx-ai regex dateparser\n",
        "\n",
        "# 1) ---- CREDENTIALS & MODEL ----\n",
        "WATSONX_API_KEY = \"B5EQe7EQckE5N1eLMtaAfGi1Gr9rOqGjyyGLN8MMispB\"\n",
        "WATSONX_URL     = \"https://us-south.ml.cloud.ibm.com\"\n",
        "WATSONX_PROJECT_ID = \"\"\n",
        "WATSONX_SPACE_ID   = \"1f66bc7c-f805-476f-92f6-1d6ebd561f15\"\n",
        "MODEL_ID = \"meta-llama/llama-3-3-70b-instruct\"\n",
        "\n",
        "# 2) ---- RUNTIME SETTINGS ----\n",
        "TEMPERATURE = 0.3\n",
        "TOP_P = 0.9\n",
        "MAX_NEW_TOKENS = 300\n",
        "\n",
        "# 3) ---- IMPORTS & MODEL SETUP ----\n",
        "import os, json, re, os.path, io\n",
        "from typing import List, Dict, Optional\n",
        "from urllib.parse import urlparse\n",
        "import dateparser\n",
        "\n",
        "assert WATSONX_API_KEY and WATSONX_URL and (WATSONX_PROJECT_ID or WATSONX_SPACE_ID), \\\n",
        "    \"Missing WATSONX_API_KEY/WATSONX_URL and either PROJECT_ID or SPACE_ID.\"\n",
        "\n",
        "os.environ[\"WATSONX_API_KEY\"]  = WATSONX_API_KEY\n",
        "os.environ[\"WATSONX_URL\"]      = WATSONX_URL\n",
        "if WATSONX_SPACE_ID:\n",
        "    os.environ[\"WATSONX_SPACE_ID\"]   = WATSONX_SPACE_ID\n",
        "    os.environ.pop(\"WATSONX_PROJECT_ID\", None)\n",
        "else:\n",
        "    os.environ[\"WATSONX_PROJECT_ID\"] = WATSONX_PROJECT_ID\n",
        "    os.environ.pop(\"WATSONX_SPACE_ID\", None)\n",
        "\n",
        "from ibm_watsonx_ai import Credentials\n",
        "from ibm_watsonx_ai.foundation_models import Model\n",
        "\n",
        "creds = Credentials(url=os.environ[\"WATSONX_URL\"], api_key=os.environ[\"WATSONX_API_KEY\"])\n",
        "params = {\"decoding_method\":\"sample\",\"temperature\":float(TEMPERATURE),\"top_p\":float(TOP_P),\"max_new_tokens\":int(MAX_NEW_TOKENS)}\n",
        "\n",
        "mdl_kwargs = {}\n",
        "if os.getenv(\"WATSONX_SPACE_ID\"):\n",
        "    mdl_kwargs[\"space_id\"] = os.environ[\"WATSONX_SPACE_ID\"]\n",
        "else:\n",
        "    mdl_kwargs[\"project_id\"] = os.environ[\"WATSONX_PROJECT_ID\"]\n",
        "\n",
        "wxa_model = Model(model_id=MODEL_ID, credentials=creds, params=params, **mdl_kwargs)\n",
        "\n",
        "def watsonx_generate(prompt: str) -> str:\n",
        "    \"\"\"Try chat; fall back to non-chat generate_text; return plain string.\"\"\"\n",
        "    try:\n",
        "        if hasattr(wxa_model, \"start_chat\"):\n",
        "            chat = wxa_model.start_chat()\n",
        "            resp = chat.send_message(prompt)\n",
        "            text = getattr(resp, \"message\", None) or getattr(resp, \"generated_text\", None)\n",
        "            if isinstance(text, str) and text.strip():\n",
        "                return text\n",
        "            if isinstance(resp, dict):\n",
        "                return resp.get(\"generated_text\") or resp.get(\"message\") or json.dumps(resp)\n",
        "    except Exception:\n",
        "        pass\n",
        "    out = wxa_model.generate_text(prompt=prompt)\n",
        "    if isinstance(out, str):\n",
        "        return out\n",
        "    if isinstance(out, dict):\n",
        "            if \"results\" in out and out[\"results\"]:\n",
        "                cand = out[\"results\"][0].get(\"generated_text\") or out[\"results\"][0].get(\"text\")\n",
        "                if cand:\n",
        "                    return cand\n",
        "            return out.get(\"generated_text\") or json.dumps(out)\n",
        "    return str(out)\n",
        "\n",
        "# 4) ---- HELPERS ----\n",
        "def trim_tweet(text: str, max_chars: int = 280) -> str:\n",
        "    t = re.sub(r\"\\s+\", \" \", text.strip())\n",
        "    if len(t) <= max_chars:\n",
        "        return t\n",
        "    cut = t[:max_chars]\n",
        "    idx = max(cut.rfind(\".\"), cut.rfind(\";\"), cut.rfind(\",\"), cut.rfind(\" \"))\n",
        "    if idx > max_chars * 0.6:\n",
        "        return cut[:idx].rstrip()\n",
        "    return cut.rstrip()\n",
        "\n",
        "URL_COMPANY_MAP = {\n",
        "    \"gene.com\": \"Genentech\",\n",
        "    \"roche.com\": \"Roche\",\n",
        "    \"novartis.com\": \"Novartis\",\n",
        "    \"pfizer.com\": \"Pfizer\",\n",
        "    \"astrazeneca.com\": \"AstraZeneca\",\n",
        "    \"lilly.com\": \"Eli Lilly\",\n",
        "    \"merck.com\": \"Merck\",\n",
        "    \"sanofi.com\": \"Sanofi\",\n",
        "    \"gilead.com\": \"Gilead\",\n",
        "    \"bms.com\": \"BMS\",\n",
        "    \"amgen.com\": \"Amgen\",\n",
        "    \"gsk.com\": \"GSK\",\n",
        "    \"bayer.com\": \"Bayer\",\n",
        "    \"takeda.com\": \"Takeda\",\n",
        "    \"boehringer-ingelheim.com\": \"Boehringer Ingelheim\",\n",
        "    \"beigene.com\": \"BeiGene\",\n",
        "    \"seagen.com\": \"Seagen\",\n",
        "    \"sermonixpharma.com\": \"Sermonix Pharma\",\n",
        "    \"janssen.com\": \"Janssen\",\n",
        "    \"johnsonandjohnson.com\": \"Johnson & Johnson\",\n",
        "    \"jnj.com\": \"Johnson & Johnson\",\n",
        "    \"abbvie.com\": \"AbbVie\",\n",
        "    \"abbott.com\": \"Abbott\",\n",
        "    \"biogen.com\": \"Biogen\",\n",
        "    \"celgene.com\": \"Celgene\",\n",
        "    \"regeneron.com\": \"Regeneron\",\n",
        "    \"modernatx.com\": \"Moderna\",\n",
        "}\n",
        "\n",
        "def company_from_url(url: str) -> Optional[str]:\n",
        "    try:\n",
        "        host = urlparse(url).netloc.lower()\n",
        "        for prefix in (\"www.\", \"amp.\", \"m.\", \"news.\", \"media.\", \"investor.\"):\n",
        "            if host.startswith(prefix):\n",
        "                host = host[len(prefix):]\n",
        "        if host in URL_COMPANY_MAP:\n",
        "            return URL_COMPANY_MAP[host]\n",
        "        parts = host.split(\".\")\n",
        "        brand = parts[-2] if len(parts) >= 2 else parts[0]\n",
        "        brand = brand.replace(\"-\", \" \").strip()\n",
        "        return brand.capitalize() if brand else None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# Indications (keep distinct)\n",
        "BREAST_CANCER_INDICATIONS = [\n",
        "    \"breast cancer\", \"metastatic breast cancer\", \"early breast cancer\", \"advanced breast cancer\",\n",
        "    \"HER2-positive breast cancer\", \"HER2+ breast cancer\", \"triple negative breast cancer\", \"TNBC\",\n",
        "    \"hormone receptor positive breast cancer\", \"HR+ breast cancer\", \"ER+ breast cancer\", \"PR+ breast cancer\",\n",
        "    \"inflammatory breast cancer\", \"ductal carcinoma\", \"lobular carcinoma\", \"locally advanced breast cancer\"\n",
        "]\n",
        "\n",
        "def extract_publication_date(content: str, headline: str) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Extract publication date prioritizing the start of content (plus headline).\n",
        "    \"\"\"\n",
        "    text_for_date = (headline + \" \" + content[:2000])\n",
        "    date_patterns = [\n",
        "        r'\\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},?\\s+\\d{4}\\b',\n",
        "        r'\\b\\d{1,2}\\s+(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{4}\\b',\n",
        "        r'\\b\\d{4}-\\d{2}-\\d{2}\\b',\n",
        "        r'\\b\\d{1,2}/\\d{1,2}/\\d{4}\\b',\n",
        "        r'\\b\\d{1,2}-\\d{1,2}-\\d{4}\\b',\n",
        "        r'\\b(?:Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday),\\s+(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},?\\s+\\d{4}\\b'\n",
        "    ]\n",
        "    for pattern in date_patterns:\n",
        "        matches = re.findall(pattern, text_for_date, re.IGNORECASE)\n",
        "        for match in matches:\n",
        "            parsed_date = dateparser.parse(match)\n",
        "            if parsed_date:\n",
        "                return parsed_date.strftime('%Y-%m-%d')\n",
        "    # LLM fallback\n",
        "    try:\n",
        "        prompt = f\"\"\"\n",
        "Extract the publication date from the following text. Return ONLY the date in YYYY-MM-DD format or \"Not found\" if no clear date.\n",
        "\n",
        "TEXT:\n",
        "\\\"\\\"\\\"{text_for_date[:3000]}\\\"\\\"\\\"\n",
        "\n",
        "\n",
        "Date (YYYY-MM-DD or \"Not found\"):\n",
        "\"\"\"\n",
        "        response = watsonx_generate(prompt).strip()\n",
        "        if response and response != \"Not found\" and re.match(r'^\\d{4}-\\d{2}-\\d{2}$', response):\n",
        "            return response\n",
        "    except Exception:\n",
        "        pass\n",
        "    return None\n",
        "\n",
        "def extract_indication(headline: str, content: str) -> List[str]:\n",
        "    text_all = (headline + \" \" + content).lower()\n",
        "    found = []\n",
        "    for indication in BREAST_CANCER_INDICATIONS:\n",
        "        if indication.lower() in text_all:\n",
        "            found.append(indication)\n",
        "    # LLM assist for other disease mentions\n",
        "    try:\n",
        "        prompt = f\"\"\"\n",
        "Extract all specific disease indications, cancer subtypes, or medical conditions mentioned in this text.\n",
        "Return ONLY a JSON array of strings with the specific indications found.\n",
        "\n",
        "TEXT:\n",
        "\\\"\\\"\\\"{text_all[:4000]}\\\"\\\"\\\"\n",
        "\n",
        "\n",
        "JSON array:\n",
        "\"\"\"\n",
        "        response = watsonx_generate(prompt).strip()\n",
        "        match = re.search(r'\\[.*\\]', response, flags=re.S)\n",
        "        if match:\n",
        "            llm_list = json.loads(match.group(0))\n",
        "            if isinstance(llm_list, list):\n",
        "                for ind in llm_list:\n",
        "                    if isinstance(ind, str) and ind.strip():\n",
        "                        if any(k in ind.lower() for k in ['cancer','carcinoma','tumor','neoplasm','metastatic','advanced']):\n",
        "                            found.append(ind.strip())\n",
        "    except Exception:\n",
        "        pass\n",
        "    # distinct, preserve order\n",
        "    seen=set(); out=[]\n",
        "    for x in found:\n",
        "        if x not in seen:\n",
        "            seen.add(x); out.append(x)\n",
        "    return out\n",
        "\n",
        "def llm_extract_drug_names_only(headline: str, content: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Extract drug/product names that appear verbatim in text.\n",
        "    EXCLUDES therapy classes (e.g., chemotherapy, immunotherapy, targeted therapy).\n",
        "    \"\"\"\n",
        "    snippet = (headline + \"\\n\" + content).strip()\n",
        "    if len(snippet) > 5000:\n",
        "        snippet = snippet[:5000]\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Extract drug or product names that appear **verbatim** in the text below.\n",
        "\n",
        "RULES:\n",
        "- Return ONLY names that literally appear in the text (headline or content).\n",
        "- If a brand and its generic appear like \"Itovebi (inavolisib)\", return BOTH as separate strings.\n",
        "- EXCLUDE therapy class terms like \"chemotherapy\", \"immunotherapy\", \"targeted therapy\", \"radiotherapy\", etc.\n",
        "- Return a pure JSON array of strings (no commentary).\n",
        "\n",
        "TEXT:\n",
        "\\\"\\\"\\\"{snippet}\\\"\\\"\\\"\n",
        "\n",
        "\n",
        "Return JSON array:\n",
        "\"\"\"\n",
        "    raw = watsonx_generate(prompt).strip()\n",
        "    match = re.search(r\"\\[\\s*(?:\\\".*?\\\")\\s*(?:,.*?)*\\]\", raw, flags=re.S)\n",
        "    items = []\n",
        "    if match:\n",
        "        try:\n",
        "            items = json.loads(match.group(0))\n",
        "        except Exception:\n",
        "            items = []\n",
        "    if not isinstance(items, list):\n",
        "        items = []\n",
        "    items = [s.strip() for s in items if isinstance(s, str) and s.strip()]\n",
        "\n",
        "    # Validate literal presence (normalize ™/®). No therapy classes allowed.\n",
        "    low = snippet.lower()\n",
        "    banned = {\"chemotherapy\",\"immunotherapy\",\"endocrine therapy\",\"hormonal therapy\",\"targeted therapy\",\n",
        "              \"radiation therapy\",\"radiotherapy\",\"adjuvant therapy\",\"neoadjuvant therapy\"}\n",
        "    validated = []\n",
        "    for name in items:\n",
        "        norm = re.sub(r\"[™®]\", \"\", name).strip()\n",
        "        if norm and (norm.lower() in low) and (norm.lower() not in banned):\n",
        "            validated.append(name)\n",
        "\n",
        "    # Dedup, preserve order\n",
        "    seen=set(); out=[]\n",
        "    for x in validated:\n",
        "        k=x.lower()\n",
        "        if k not in seen:\n",
        "            seen.add(k); out.append(x)\n",
        "    return out\n",
        "\n",
        "# ===== Trial phase & study name extraction =====\n",
        "ROMAN_MAP = {\"I\":\"1\",\"II\":\"2\",\"III\":\"3\",\"IV\":\"4\",\"V\":\"5\"}\n",
        "\n",
        "def extract_trial_phase(headline: str, content: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Normalize to forms like 'Phase 1', 'Phase 2/3', 'Phase 3b'.\n",
        "    \"\"\"\n",
        "    text = f\"{headline}\\n{content}\"\n",
        "    phases = set()\n",
        "    patterns = [\n",
        "        r'\\b[Pp]hase\\s*(I{1,3}V?|V|1|2|3|4)(?:\\s*[/\\-]\\s*(I{1,3}V?|V|1|2|3|4))?\\s*([a-dA-D])?\\b',\n",
        "        r'\\b[Pp](?:h|H)?\\s*(\\d)(?:\\s*/\\s*(\\d))?\\b',\n",
        "        r'\\b(?:[Pp]hase)?\\s*(\\d)\\s*/\\s*(\\d)\\b',\n",
        "        r'\\b[Pp]hase\\s*(I{1,3})([a-dA-D])\\b',\n",
        "        r'\\b[Pp](\\d)([a-dA-D])\\b'\n",
        "    ]\n",
        "    for pat in patterns:\n",
        "        for m in re.finditer(pat, text):\n",
        "            grp = [g for g in m.groups() if g]\n",
        "            if not grp:\n",
        "                continue\n",
        "            nums=[]; suffix=\"\"\n",
        "            for g in grp:\n",
        "                G=g.upper()\n",
        "                if G in ROMAN_MAP: nums.append(ROMAN_MAP[G])\n",
        "                elif re.fullmatch(r\"[IVX]+\", G): nums.append(str(len(G)))  # coarse\n",
        "                elif re.fullmatch(r\"\\d\", G): nums.append(G)\n",
        "                elif re.fullmatch(r\"[A-D]\", G): suffix = G.lower()\n",
        "            if len(nums)==1:\n",
        "                phases.add(f\"Phase {nums[0]}{suffix}\")\n",
        "            elif len(nums)>=2:\n",
        "                phases.add(f\"Phase {nums[0]}/{nums[1]}{suffix}\")\n",
        "\n",
        "    # LLM fallback\n",
        "    if not phases:\n",
        "        try:\n",
        "            snippet = (headline + \"\\n\" + content)[:4500]\n",
        "            prompt = f\"\"\"\n",
        "From the text, extract the clinical trial PHASE if mentioned.\n",
        "Return ONLY a JSON array of normalized strings like \"Phase 1\", \"Phase 2/3\", \"Phase 3b\".\n",
        "If none, return [].\n",
        "\n",
        "TEXT:\n",
        "\\\"\\\"\\\"{snippet}\\\"\\\"\\\"\n",
        "\n",
        "\n",
        "JSON array:\n",
        "\"\"\"\n",
        "            resp = watsonx_generate(prompt).strip()\n",
        "            mm = re.search(r'\\[.*\\]', resp, flags=re.S)\n",
        "            if mm:\n",
        "                arr = json.loads(mm.group(0))\n",
        "                if isinstance(arr, list):\n",
        "                    for x in arr:\n",
        "                        if isinstance(x, str) and x.strip().lower().startswith(\"phase\"):\n",
        "                            phases.add(x.strip())\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    out=[]; seen=set()\n",
        "    for p in phases:\n",
        "        if p not in seen:\n",
        "            seen.add(p); out.append(p)\n",
        "    return out\n",
        "\n",
        "STUDY_NAME_HINTS = [\n",
        "    \"keynote\",\"tropion\",\"destiny\",\"checkmate\",\"impower\",\"impassion\",\n",
        "    \"monarch\",\"olympiad\",\"ascend\",\"clarity\",\"polo\",\"bright\",\"palace\",\"compas\",\"compass\"\n",
        "]\n",
        "\n",
        "def extract_trial_names(headline: str, content: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Return only PROPER study names (e.g., 'KEYNOTE-522', 'TROPION-Breast01').\n",
        "    Never return lone roman numerals or plain 'phase'.\n",
        "    \"\"\"\n",
        "    text = f\"{headline}\\n{content}\"\n",
        "    names=set()\n",
        "\n",
        "    # Strict patterns that require letters (prevents capturing plain 'III')\n",
        "    pats = [\n",
        "        r'[\"“]?([A-Z][A-Z0-9]+(?:[-–][A-Za-z0-9]+){0,3})[\"”]?\\s+(?:trial|study)\\b',\n",
        "        r'\\b(?:trial|study)\\s+[\"“]?([A-Z][A-Z0-9]+(?:[-–][A-Za-z0-9]+){0,3})[\"”]?\\b',\n",
        "        r'[\"“]([A-Za-z][^\"”]{2,80})[\"”]\\s+(?:trial|study)\\b',\n",
        "    ]\n",
        "    for pat in pats:\n",
        "        for m in re.finditer(pat, text):\n",
        "            cand = re.sub(r'\\s+', ' ', m.group(1).strip())\n",
        "            # Require at least one letter AND at least one letter or digit after a hyphen to resemble real study codes\n",
        "            if re.search(r'[A-Za-z]', cand) and (re.search(r'-', cand) or len(cand) >= 4):\n",
        "                # Exclude pure roman numerals\n",
        "                if not re.fullmatch(r'[IVX]+', cand, flags=re.I):\n",
        "                    names.add(cand)\n",
        "\n",
        "    # Quoted anywhere with window and having letters\n",
        "    for m in re.finditer(r'[\"“]([^\"”]{3,80})[\"”]', text):\n",
        "        span_start, span_end = m.span()\n",
        "        window = text[max(0, span_start-60):min(len(text), span_end+60)]\n",
        "        if re.search(r'\\b(trial|study)\\b', window, flags=re.I):\n",
        "            cand = re.sub(r'\\s+', ' ', m.group(1).strip())\n",
        "            if re.search(r'[A-Za-z]', cand) and not re.fullmatch(r'[IVX]+', cand, flags=re.I):\n",
        "                names.add(cand)\n",
        "\n",
        "    # LLM fallback, but keep STRICT acceptance\n",
        "    try:\n",
        "        snippet = text[:4500]\n",
        "        prompt = f\"\"\"\n",
        "If the text mentions a named clinical study/trial (e.g., KEYNOTE-522, TROPION-Breast01), return ONLY a pure JSON array of those names.\n",
        "If none, return [].\n",
        "\n",
        "TEXT:\n",
        "\\\"\\\"\\\"{snippet}\\\"\\\"\\\"\n",
        "\n",
        "\n",
        "JSON array:\n",
        "\"\"\"\n",
        "        resp = watsonx_generate(prompt).strip()\n",
        "        mm = re.search(r'\\[.*\\]', resp, flags=re.S)\n",
        "        if mm:\n",
        "            arr = json.loads(mm.group(0))\n",
        "            if isinstance(arr, list):\n",
        "                for cand in arr:\n",
        "                    if isinstance(cand, str):\n",
        "                        c = re.sub(r'\\s+', ' ', cand.strip())\n",
        "                        if c and re.search(r'[A-Za-z]', c) and not re.fullmatch(r'[IVX]+', c, flags=re.I):\n",
        "                            names.add(c)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    out=[]; seen=set()\n",
        "    for n in names:\n",
        "        if n not in seen:\n",
        "            seen.add(n); out.append(n)\n",
        "    return out\n",
        "\n",
        "def llm_summary_280(headline: str, content: str) -> str:\n",
        "    snippet = (headline + \"\\n\" + content).strip()\n",
        "    if len(snippet) > 6000:\n",
        "        snippet = snippet[:6000]\n",
        "    prompt = f\"\"\"\n",
        "Write ONE tweet-style summary (<= 280 characters) strictly based on the article text below.\n",
        "REQUIREMENTS:\n",
        "- Mention concrete specifics present in the text (e.g., drug name(s), indication, setting/line, and endpoint if mentioned).\n",
        "- No emojis, no hashtags, no marketing language, no invented details.\n",
        "- One concise sentence or two short clauses.\n",
        "\n",
        "TEXT:\n",
        "\\\"\\\"\\\"{snippet}\\\"\\\"\\\"\n",
        "\n",
        "\n",
        "Tweet (<=280 chars):\n",
        "\"\"\"\n",
        "    raw = watsonx_generate(prompt)\n",
        "    tweet = raw.splitlines()[0].strip() if raw else \"\"\n",
        "    return trim_tweet(tweet, max_chars=280)\n",
        "\n",
        "# 5) ---- PROCESSOR (STRICT SCHEMA) ----\n",
        "def process_articles(articles: List[Dict]) -> List[Dict]:\n",
        "    results = []\n",
        "    for art in articles:\n",
        "        headline = (art.get(\"headline\") or \"\").strip()\n",
        "        url = (art.get(\"link\") or art.get(\"url\") or \"\").strip()\n",
        "        content = (art.get(\"content\") or \"\").strip()\n",
        "\n",
        "        # Required fields\n",
        "        published_date = extract_publication_date(content, headline)\n",
        "        indications = extract_indication(headline, content)\n",
        "        drug_names = llm_extract_drug_names_only(headline, content)\n",
        "        trial_phases = extract_trial_phase(headline, content)\n",
        "        trial_names = extract_trial_names(headline, content)\n",
        "        summary = llm_summary_280(headline, content)\n",
        "        company_name = company_from_url(url)\n",
        "\n",
        "        # Build EXACTLY the requested output shape\n",
        "        record = {\n",
        "            \"published_date\": published_date,      # 1) date extracted (from start area)\n",
        "            \"content\": content,                    # 2) full content\n",
        "            \"entities\": {                          # 3) entities bundle\n",
        "                \"drug_names\": drug_names,\n",
        "                \"company_name\": company_name,\n",
        "                \"trial_phases\": trial_phases,\n",
        "                \"trial_names\": trial_names,\n",
        "                \"indications\": list(dict.fromkeys(indications))  # distinct\n",
        "            },\n",
        "            \"summary_280\": summary,                # 4) summary <= 280 chars\n",
        "            \"url\": url,                            # 5) actual url\n",
        "            \"headline\": headline                   # 6) headline\n",
        "        }\n",
        "        results.append(record)\n",
        "    return results\n",
        "\n",
        "print(\"✅ watsonx model ready:\", MODEL_ID)\n",
        "\n",
        "# 6) ---- LOAD INPUT JSON FROM LOCAL FILE IN COLAB ----\n",
        "INPUT_FILENAME = \"breast_cancer_articles_with_pdf_20251019_203237.json\"\n",
        "\n",
        "import glob, json\n",
        "if not os.path.exists(INPUT_FILENAME):\n",
        "    json_files = glob.glob(\"breast_cancer_articles_with_pdf_*.json\")\n",
        "    if json_files:\n",
        "        INPUT_FILENAME = json_files[0]\n",
        "        print(f\"Using found file: {INPUT_FILENAME}\")\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Could not find {INPUT_FILENAME} or similar breast_cancer_articles_with_pdf_*.json files\")\n",
        "\n",
        "print(\"Reading:\", INPUT_FILENAME)\n",
        "with open(INPUT_FILENAME, \"r\", encoding=\"utf-8\") as f:\n",
        "    articles_in = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(articles_in)} articles\")\n",
        "\n",
        "# 7) ---- RUN with INCREMENTAL WRITES ----\n",
        "print(\"Processing articles with incremental writes...\")\n",
        "\n",
        "OUTPUT_JSON = \"enhanced_article_analysis.json\"   # rolling snapshot (array)\n",
        "OUTPUT_NDJSON = \"enhanced_article_analysis.ndjson\"  # one record per line\n",
        "\n",
        "# Start fresh files\n",
        "open(OUTPUT_NDJSON, \"w\", encoding=\"utf-8\").close()\n",
        "processed_so_far: List[Dict] = []\n",
        "\n",
        "def write_incremental(current_list: List[Dict], last_record: Dict):\n",
        "    \"\"\"Append one line to NDJSON and also overwrite the JSON array file atomically.\"\"\"\n",
        "    with open(OUTPUT_NDJSON, \"a\", encoding=\"utf-8\") as g:\n",
        "        g.write(json.dumps(last_record, ensure_ascii=False) + \"\\n\")\n",
        "        g.flush()\n",
        "        os.fsync(g.fileno())\n",
        "    tmp = OUTPUT_JSON + \".tmp\"\n",
        "    with open(tmp, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(current_list, f, ensure_ascii=False, indent=2)\n",
        "        f.flush()\n",
        "        os.fsync(f.fileno())\n",
        "    os.replace(tmp, OUTPUT_JSON)\n",
        "\n",
        "processed_count = 0\n",
        "for rec in process_articles(articles_in):\n",
        "    processed_so_far.append(rec)\n",
        "    write_incremental(processed_so_far, rec)\n",
        "    processed_count += 1\n",
        "    if processed_count % 10 == 0:\n",
        "        print(f\"... {processed_count} / {len(articles_in)} done\")\n",
        "\n",
        "print(f\"Processed {processed_count} articles\")\n",
        "print(\"Sample output preview:\")\n",
        "print(json.dumps(processed_so_far[:1], ensure_ascii=False, indent=2))\n",
        "\n",
        "# 8) ---- STATS (optional)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"EXTRACTION STATS\")\n",
        "print(\"=\"*50)\n",
        "articles_with_date = sum(1 for a in processed_so_far if a.get('published_date'))\n",
        "articles_with_drugs = sum(1 for a in processed_so_far if a[\"entities\"].get('drug_names'))\n",
        "articles_with_company = sum(1 for a in processed_so_far if a[\"entities\"].get('company_name'))\n",
        "articles_with_trial_phase = sum(1 for a in processed_so_far if a[\"entities\"].get('trial_phases'))\n",
        "articles_with_trial_name = sum(1 for a in processed_so_far if a[\"entities\"].get('trial_names'))\n",
        "articles_with_indications = sum(1 for a in processed_so_far if a[\"entities\"].get('indications'))\n",
        "print(f\"Total: {len(processed_so_far)}\")\n",
        "print(f\"Published date: {articles_with_date}\")\n",
        "print(f\"Drug names: {articles_with_drugs}\")\n",
        "print(f\"Company name: {articles_with_company}\")\n",
        "print(f\"Trial phases: {articles_with_trial_phase}\")\n",
        "print(f\"Trial names: {articles_with_trial_name}\")\n",
        "print(f\"Indications: {articles_with_indications}\")\n",
        "\n",
        "# 9) ---- DOWNLOAD ----\n",
        "from google.colab import files\n",
        "files.download(OUTPUT_JSON)\n",
        "files.download(OUTPUT_NDJSON)\n",
        "\n",
        "print(f\"\\n✅ Live progress written to {OUTPUT_NDJSON}\")\n",
        "print(f\"✅ Cumulative array written to {OUTPUT_JSON} and downloaded\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FE1KN1yC7TSI",
        "outputId": "bc61e220-9251-4fc6-dd44-ef140c4d9b05"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ibm-watsonx-ai in /usr/local/lib/python3.12/dist-packages (1.4.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (2025.9.18)\n",
            "Requirement already satisfied: dateparser in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from ibm-watsonx-ai) (2.32.4)\n",
            "Requirement already satisfied: httpx<0.29,>=0.27 in /usr/local/lib/python3.12/dist-packages (from ibm-watsonx-ai) (0.28.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.12/dist-packages (from ibm-watsonx-ai) (2.5.0)\n",
            "Requirement already satisfied: pandas<2.3.0,>=0.24.2 in /usr/local/lib/python3.12/dist-packages (from ibm-watsonx-ai) (2.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from ibm-watsonx-ai) (2025.10.5)\n",
            "Requirement already satisfied: lomond in /usr/local/lib/python3.12/dist-packages (from ibm-watsonx-ai) (0.3.3)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from ibm-watsonx-ai) (0.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from ibm-watsonx-ai) (25.0)\n",
            "Requirement already satisfied: ibm-cos-sdk<2.15.0,>=2.12.0 in /usr/local/lib/python3.12/dist-packages (from ibm-watsonx-ai) (2.14.3)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from ibm-watsonx-ai) (5.5.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from dateparser) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2024.2 in /usr/local/lib/python3.12/dist-packages (from dateparser) (2025.2)\n",
            "Requirement already satisfied: tzlocal>=0.2 in /usr/local/lib/python3.12/dist-packages (from dateparser) (5.3.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<0.29,>=0.27->ibm-watsonx-ai) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<0.29,>=0.27->ibm-watsonx-ai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<0.29,>=0.27->ibm-watsonx-ai) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<0.29,>=0.27->ibm-watsonx-ai) (0.16.0)\n",
            "Requirement already satisfied: ibm-cos-sdk-core==2.14.3 in /usr/local/lib/python3.12/dist-packages (from ibm-cos-sdk<2.15.0,>=2.12.0->ibm-watsonx-ai) (2.14.3)\n",
            "Requirement already satisfied: ibm-cos-sdk-s3transfer==2.14.3 in /usr/local/lib/python3.12/dist-packages (from ibm-cos-sdk<2.15.0,>=2.12.0->ibm-watsonx-ai) (2.14.3)\n",
            "Requirement already satisfied: jmespath<=1.0.1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from ibm-cos-sdk<2.15.0,>=2.12.0->ibm-watsonx-ai) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas<2.3.0,>=0.24.2->ibm-watsonx-ai) (2.0.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<2.3.0,>=0.24.2->ibm-watsonx-ai) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7.0->dateparser) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->ibm-watsonx-ai) (3.4.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<0.29,>=0.27->ibm-watsonx-ai) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<0.29,>=0.27->ibm-watsonx-ai) (4.15.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/ibm_watsonx_ai/foundation_models/model.py:106: DeprecationWarning: The `Model` class is deprecated and will be removed in a future release. Please use the `ModelInference` class instead. To update your imports, use: `from ibm_watsonx_ai.foundation_models import ModelInference`.\n",
            "  warn(model_class_deprecated_warning, category=DeprecationWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ watsonx model ready: meta-llama/llama-3-3-70b-instruct\n",
            "Reading: breast_cancer_articles_with_pdf_20251019_203237.json\n",
            "Loaded 150 articles\n",
            "Processing articles with incremental writes...\n",
            "... 10 / 150 done\n",
            "... 20 / 150 done\n",
            "... 30 / 150 done\n",
            "... 40 / 150 done\n",
            "... 50 / 150 done\n",
            "... 60 / 150 done\n",
            "... 70 / 150 done\n",
            "... 80 / 150 done\n",
            "... 90 / 150 done\n",
            "... 100 / 150 done\n",
            "... 110 / 150 done\n",
            "... 120 / 150 done\n",
            "... 130 / 150 done\n",
            "... 140 / 150 done\n",
            "... 150 / 150 done\n",
            "Processed 150 articles\n",
            "Sample output preview:\n",
            "[\n",
            "  {\n",
            "    \"published_date\": null,\n",
            "    \"content\": \"Positive results from the TROPION-Breast02 Phase III trial showedDatroway(datopotamab deruxtecan) demonstrated a statistically significant and clinically meaningful improvement for the dual primary endpoints of overall survival (OS) and progression-free survival (PFS) compared to investigator's choice of chemotherapy as 1st-line treatment for patients with locally recurrent inoperable or metastatic triple-negative breast cancer (TNBC) for whom immunotherapy was not an option. These late-breaking results will be presented today during a Proffered Paper session at the 2025 European Society for Medical Oncology (ESMO) Congress in Berlin, Germany (abstract #LBA21). Datrowaydemonstrated a 5.0-month improvement in median OS compared to chemotherapy (hazard ratio [HR] 0.79; 95% confidence interval [CI] 0.64-0.98; p=0.0291). Median OS was 23.7 months for patients treated withDatrowayversus 18.7 months for those treated with chemotherapy. Datrowayreduced the risk of disease progression or death by 43% compared to chemotherapy (HR 0.57; 95% CI 0.47-0.69; p<0.0001) as assessed by blinded independent central review (BICR). Median PFS was 10.8 months for patients treated withDatrowayversus 5.6 months for those treated with chemotherapy. In addition to patients whose tumours did not express PD-L1, TROPION-Breast02 enrolled patients with PD-L1 expressing tumours for whom immunotherapy was not an option due to other factors. Rebecca Dent, MD, FRCP, Professor and Deputy Chief Executive Officer, National Cancer Centre Singapore, and principal investigator for the trial, said: “In TROPION-Breast02, datopotamab deruxtecan meaningfully extended patients’ lives and nearly doubled their time without disease progression. These are significant outcomes for patients with metastatic triple-negative breast cancer who are not suitable candidates for immunotherapy, and remarkable results considering the trial included a subset of patients with highly aggressive disease who are often excluded from research in this setting.” Susan Galbraith, Executive Vice President, Oncology Haematology R&D, AstraZeneca, said: “The TROPION-Breast02 results show for the first time that these triple-negative breast cancer patients may have an alternative to chemotherapy in the 1st-line setting that can both delay the progression of their disease and prolong their lives. ForDatrowayto have so significantly improved patient outcomes in the 1st-line metastatic setting as monotherapy also gives us great confidence in its potential in combination withImfinzi, and in the early-stage, potentially curative setting where our next studies are ongoing.” Ken Takeshita, Global Head, R&D, Daiichi Sankyo, said: “Patients with metastatic triple-negative breast cancer have one of the worst prognoses of any breast cancer subtype, and for those who are not candidates for immunotherapy, chemotherapy has long been the 1st-line standard of care. The TROPION-Breast02 results showDatrowayhas the potential to replace traditional chemotherapy in this setting and to meaningfully improve survival of patients.” Datrowayis a specifically engineered TROP2-directed DXd antibody drug conjugate (ADC) discovered by Daiichi Sankyo and being jointly developed and commercialised by AstraZeneca and Daiichi Sankyo.  Summary of efficacy results  Datroway (n=323) ICC (n=321)  Median OS, months (95% CI) Datroway (n=323) 23.7 (19.8-25.6) ICC (n=321) 18.7 (16.0-21.8)  HR (95% CI) Datroway (n=323) 0.79 (0.64-0.98)  p-value Datroway (n=323) 0.0291  Median PFS by BICR, months (95% CI) Datroway (n=323) 10.8 (8.6-13.0) ICC (n=321) 5.6 (5.0-7.0)  HR (95% CI) Datroway (n=323) 0.57 (0.47-0.69)  p-value Datroway (n=323) <0.0001  Median PFS by investigator, months (95% CI) Datroway (n=323) 9.6 (7.4-11.2) ICC (n=321) 5.2 (4.2-5.6)  HR (95% CI) Datroway (n=323) 0.56 (0.47-0.67)  Confirmed ORR, % Datroway (n=323) 62.5 ICC (n=321) 29.3  CR, % (n) Datroway (n=323) 9.0 (29) ICC (n=321) 2.5 (8)  PR, % (n) Datroway (n=323) 53.6 (173) ICC (n=321) 26.8 (86)  Median DoR, months (95% CI) Datroway (n=323) 12.3 (9.1-15.9) ICC (n=321) 7.1 (5.6-8.9) As of 25 August 2025, data cut-off, 45 patients (14%) remained on Datroway and 8 patients (3%) on chemotherapy.BICR, blinded independent central review; CI, confidence interval; CR, complete response; DoR, duration of response; HR, hazard ratio; ICC, investigator’s choice of chemotherapy; ORR, objective response rate; OS, overall survival; PFS, progression-free survival; PR, partial response Patients receivingDatrowaywere on treatment more than twice as long as those receiving chemotherapy (median duration of treatment of 8.5 versus 4.1 months) and experienced a lower rate of treatment-related adverse events (TRAEs) associated with discontinuation (4% versus 7%). Grade 3 or higher TRAEs occurred in 33% and 29% of patients in theDatrowayand chemotherapy arms, respectively. The most common Grade 3 or higher TRAEs were neutropenia (3%, 13%), stomatitis (8%, 0%), leukopenia (<1%, 4%), fatigue (3%, 3%), vomiting (1%, <1%), anaemia (2%, 3%), alopecia (0%, <1%), peripheral neuropathy (0%, 2%), dry eye (1%, 0%), nausea (<1%, <1%), decreased appetite (<1%, <1%) and constipation (<1%, 0%). There was one Grade 5 interstitial lung disease (ILD) event in theDatrowayarm adjudicated as drug-related by an independent committee. This event was characterised as Grade 3 pneumonitis and cause of death was attributed to disease progression by the treating investigator. AstraZeneca and Daiichi Sankyo will also present updated results from the BEGONIA Phase Ib/II trial at ESMO showingDatrowayin combination withImfinzi(durvalumab) continued to demonstrate robust anti-tumour activity as 1st-line treatment for patients with metastatic TNBC across PD-L1 expression levels and specifically in those with high PD-L1-expressing tumours. These results will be presented on Monday, 20 October (abstract #555MO). AstraZeneca and Daiichi Sankyo are evaluatingDatrowayacross stages and treatment settings of TNBC in three additional Phase III trials.TROPION-Breast03is evaluatingDatrowaywith or withoutImfinziin patients with Stage I-III TNBC with residual invasive disease after neoadjuvant systemic therapy.TROPION-Breast04is evaluating neoadjuvantDatrowayplusImfinziin patients with Stage II-III triple-negative or hormone receptor (HR)-low, HER2-low or -negative breast cancer.TROPION-Breast05is evaluating 1st-lineDatrowaywith or withoutImfinziin patients with metastatic TNBC whose tumours express PD-L1. Notes Triple-negative breast cancerTNBC accounts for approximately 15% of all breast cancer cases, with an estimated 345,000 diagnoses globally each year.1,2TNBC is diagnosed more frequently in younger and premenopausal women, and is more prevalent in Black and Hispanic women.3-5Metastatic TNBC is the most aggressive type of breast cancer and has one of the worst prognoses, with median OS of just 12 to 18 months and only about 14% of patients living five years following diagnosis.3,6,7 While some breast cancers may test positive for oestrogen receptors, progesterone receptors or overexpression of HER2, TNBC tests negative for all three.3Due to its aggressive nature and absence of common breast cancer receptors, TNBC is characteristically difficult to treat.3For patients with metastatic disease with PD-L1 expressing tumours, the addition of immunotherapy to chemotherapy has improved outcomes in the 1st-line setting.8,9However, for the approximately 70% of patients with metastatic TNBC who are not candidates for immunotherapy, chemotherapy remains the 1st-line standard of care.10,11 TROP2 is a protein broadly expressed in several solid tumours including TNBC.12TROP2 is associated with increased tumour progression and poor survival in patients with breast cancer.13,14 TROPION-Breast02TROPION-Breast02 is a global, multicentre, randomised, open-label Phase III trial evaluating the efficacy and safety ofDatrowayversus investigator’s choice of chemotherapy (paclitaxel, nab-paclitaxel, capecitabine, carboplatin or eribulin) in patients with previously untreated locally recurrent inoperable or metastatic TNBC for whom immunotherapy was not an option. This included patients whose tumours did not express PD-L1 as well as patients with PD-L1 expressing tumours who could not receive immunotherapy due to prior exposure in early-stage disease, comorbidities or immunotherapy not being accessible in their geography. Enrolment included patients with de novo or recurrent disease, regardless of disease-free interval, and those with poor prognostic factors such as stable brain metastases. The dual primary endpoints of TROPION-Breast02 are PFS as assessed by BICR and OS. Key secondary endpoints include PFS as assessed by investigator, objective response rate, duration of response, disease control rate, pharmacokinetics and safety. TROPION-Breast02 enrolled 644 patients at sites in Africa, Asia, Europe, North America and South America. For more information, visitClinicalTrials.gov. DatrowayDatroway(datopotamab deruxtecan; datopotamab deruxtecan-dlnk in the US only) is a TROP2-directed ADC. Designed using Daiichi Sankyo’s proprietary DXd ADC Technology,Datrowayis one of six DXd ADCs in the oncology pipeline of Daiichi Sankyo, and one of the most advanced programmes in AstraZeneca’s ADC scientific platform.Datrowayis comprised of a humanised anti-TROP2 IgG1 monoclonal antibody, developed in collaboration with Sapporo Medical University, attached to a number of topoisomerase I inhibitor payloads (an exatecan derivative, DXd) via tetrapeptide-based cleavable linkers. Datrowayis approved in more than 35 countries/regions worldwide for the treatment of adult patients with unresectable or metastatic HR-positive, HER2-negative (IHC 0, IHC 1+ or IHC 2+/ISH-) breast cancer who have received prior endocrine-based therapy and chemotherapy for unresectable or metastatic disease based on results from theTROPION-Breast01trial. Datrowayis available in the US under accelerated approval for the treatment of adult patients with locally advanced or metastaticEGFR-mutated non-small cell lung cancer (NSCLC) who have received priorEGFR-directed therapy and platinum-based chemotherapy based on results from theTROPION-Lung05andTROPION-Lung01trials. Continued approval for this indication in the US may be contingent upon verification and description of clinical benefit in a confirmatory trial.Datrowayis approved in Russia for the same population. Datrowayclinical development programmeA comprehensive global clinical development programme is underway with more than 20 trials evaluating the efficacy and safety ofDatrowayacross multiple cancers, including NSCLC, TNBC and urothelial cancer. The programme includes eight Phase III trials in lung cancer and five Phase III trials in breast cancer evaluatingDatrowayas a monotherapy and in combination with other anticancer treatments in various settings. Daiichi Sankyo collaborationAstraZeneca and Daiichi Sankyo entered into a global collaboration to jointly develop and commercialiseEnhertuinMarch 2019andDatrowayinJuly 2020, except in Japan where Daiichi Sankyo maintains exclusive rights for each ADC. Daiichi Sankyo is responsible for the manufacturing and supply ofEnhertuandDatroway. AstraZeneca in breast cancerDriven by a growing understanding of breast cancer biology, AstraZeneca is challenging, and redefining, the current clinical paradigm for how breast cancer is classified and treated to deliver even more effective treatments to patients in need – with the bold ambition to one day eliminate breast cancer as a cause of death. AstraZeneca has a comprehensive portfolio of approved and promising compounds in development that leverage different mechanisms of action to address the biologically diverse breast cancer tumour environment. WithEnhertu, AstraZeneca and Daiichi Sankyo are aiming to improve outcomes in previously treated HER2-positive, HER2-low and HER2-ultralow metastatic breast cancer, and are exploring its potential in earlier lines of treatment and in new breast cancer settings. In HR-positive breast cancer, AstraZeneca continues to improve outcomes with foundational medicinesFaslodex(fulvestrant) andZoladex(goserelin) and aims to reshape the HR-positive space with first-in-class AKT inhibitor,Truqap(capivasertib), the TROP2-directed ADC,Datroway, and next-generation oral SERD and potential new medicine camizestrant. PARP inhibitorLynparza(olaparib) is a targeted treatment option that has been studied in early and metastatic breast cancer patients with an inherited BRCA mutation. AstraZeneca with MSD (Merck & Co., Inc. in the US and Canada) continue to researchLynparzain these settings. AstraZeneca is also exploring the potential of saruparib, a potent and selective inhibitor of PARP1, in combination with camizestrant in BRCA-mutated, HR-positive, HER2-negative advanced breast cancer. To bring much-needed treatment options to patients with triple-negative breast cancer, an aggressive form of breast cancer, AstraZeneca is collaborating with Daiichi Sankyo to evaluate the potential ofDatrowayalone and in combination with immunotherapyImfinzi. AstraZeneca in oncologyAstraZeneca is leading a revolution in oncology with the ambition to provide cures for cancer in every form, following the science to understand cancer and all its complexities to discover, develop and deliver life-changing medicines to patients. The Company’s focus is on some of the most challenging cancers. It is through persistent innovation that AstraZeneca has built one of the most diverse portfolios and pipelines in the industry, with the potential to catalyse changes in the practice of medicine and transform the patient experience. AstraZeneca has the vision to redefine cancer care and, one day, eliminate cancer as a cause of death. AstraZenecaAstraZeneca (LSE/STO/Nasdaq: AZN) is a global, science-led biopharmaceutical company that focuses on the discovery, development, and commercialisation of prescription medicines in Oncology, Rare Diseases, and BioPharmaceuticals, including Cardiovascular, Renal & Metabolism, and Respiratory & Immunology. Based in Cambridge, UK, AstraZeneca’s innovative medicines are sold in more than 125 countries and used by millions of patients worldwide. Please visitastrazeneca.comand follow the Company on Social Media@AstraZeneca. ContactsFor details on how to contact the Investor Relations Team, please clickhere. For Media contacts, clickhere. References\",\n",
            "    \"entities\": {\n",
            "      \"drug_names\": [\n",
            "        \"Datroway\",\n",
            "        \"datopotamab deruxtecan\",\n",
            "        \"Imfinzi\"\n",
            "      ],\n",
            "      \"company_name\": \"AstraZeneca\",\n",
            "      \"trial_phases\": [\n",
            "        \"Phase 3\",\n",
            "        \"Phase 1b\"\n",
            "      ],\n",
            "      \"trial_names\": [\n",
            "        \"TROPION-Breast02\"\n",
            "      ],\n",
            "      \"indications\": [\n",
            "        \"breast cancer\",\n",
            "        \"metastatic breast cancer\",\n",
            "        \"advanced breast cancer\",\n",
            "        \"TNBC\",\n",
            "        \"Metastatic triple-negative breast cancer\",\n",
            "        \"Locally recurrent inoperable or metastatic triple-negative breast cancer\",\n",
            "        \"Triple-negative breast cancer\",\n",
            "        \"Breast cancer\"\n",
            "      ]\n",
            "    },\n",
            "    \"summary_280\": \"Datroway, a TROP2-directed ADC, showed a 5-month improvement in median overall survival vs chemotherapy as 1st-line treatment for metastatic triple-negative breast cancer, with median OS of 23.7 months vs 18.7 months.\",\n",
            "    \"url\": \"https://www.astrazeneca.com/content/astraz/media-centre/press-releases/2025/datroway-demonstrated-an-unprecedented-median-overall-survival-improvement-of-five-months-vs-chemotherapy-as-1st-line-treatment-for-patients-with-metastatic-triple-negative-breast-cancer-for-whom-immunotherapy-was-not-an-option-in-tropion-breast02.html\",\n",
            "    \"headline\": \"Datroway demonstrated an unprecedented median overall survival improvement of five months vs. chemotherapy as 1st-line treatment for patients with metastatic triple-negative breast cancer for whom imm\"\n",
            "  }\n",
            "]\n",
            "\n",
            "==================================================\n",
            "EXTRACTION STATS\n",
            "==================================================\n",
            "Total: 150\n",
            "Published date: 123\n",
            "Drug names: 148\n",
            "Company name: 150\n",
            "Trial phases: 145\n",
            "Trial names: 138\n",
            "Indications: 150\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6d991946-d827-4e31-95bd-ff4a4758873b\", \"enhanced_article_analysis.json\", 3515710)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0c30f60f-b2a0-43d2-a183-40a0dd3a66e5\", \"enhanced_article_analysis.ndjson\", 3485738)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Live progress written to enhanced_article_analysis.ndjson\n",
            "✅ Cumulative array written to enhanced_article_analysis.json and downloaded\n"
          ]
        }
      ]
    }
  ]
}